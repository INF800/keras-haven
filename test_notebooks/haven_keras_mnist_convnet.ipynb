{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "haven_keras_mnist_convnet",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_bh1yDChIb4"
      },
      "source": [
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zLJZIIm9hIb5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6cc863bf-ee20-47bd-a2d0-619f86714974"
      },
      "source": [
        "# Model / data parameters\n",
        "num_classes = 10\n",
        "input_shape = (28, 28, 1)\n",
        "\n",
        "# the data, split between train and test sets\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "\n",
        "# Scale images to the [0, 1] range\n",
        "x_train = x_train.astype(\"float32\") / 255\n",
        "x_test = x_test.astype(\"float32\") / 255\n",
        "# Make sure images have shape (28, 28, 1)\n",
        "x_train = np.expand_dims(x_train, -1)\n",
        "x_test = np.expand_dims(x_test, -1)\n",
        "print(\"x_train shape:\", x_train.shape)\n",
        "print(x_train.shape[0], \"train samples\")\n",
        "print(x_test.shape[0], \"test samples\")\n",
        "\n",
        "\n",
        "# convert class vectors to binary class matrices\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "\n",
        "model = keras.Sequential(\n",
        "    [\n",
        "        keras.Input(shape=input_shape),\n",
        "        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
        "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
        "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        layers.Flatten(),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(num_classes, activation=\"softmax\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "model.summary()\n",
        "\n",
        "batch_size = 128\n",
        "epochs = 15\n",
        "\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "\n",
        "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)\n",
        "\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print(\"Test loss:\", score[0])\n",
        "print(\"Test accuracy:\", score[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train shape: (60000, 28, 28, 1)\n",
            "60000 train samples\n",
            "10000 test samples\n",
            "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n",
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_6 (Conv2D)            (None, 26, 26, 32)        320       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_6 (MaxPooling2 (None, 13, 13, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 11, 11, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_7 (MaxPooling2 (None, 5, 5, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten_3 (Flatten)          (None, 1600)              0         \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 1600)              0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 10)                16010     \n",
            "=================================================================\n",
            "Total params: 34,826\n",
            "Trainable params: 34,826\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/15\n",
            "422/422 [==============================] - 4s 9ms/step - loss: 0.3676 - accuracy: 0.8899 - val_loss: 0.0788 - val_accuracy: 0.9787\n",
            "Epoch 2/15\n",
            "422/422 [==============================] - 3s 8ms/step - loss: 0.1111 - accuracy: 0.9661 - val_loss: 0.0596 - val_accuracy: 0.9830\n",
            "Epoch 3/15\n",
            "422/422 [==============================] - 3s 8ms/step - loss: 0.0848 - accuracy: 0.9740 - val_loss: 0.0464 - val_accuracy: 0.9890\n",
            "Epoch 4/15\n",
            "422/422 [==============================] - 4s 8ms/step - loss: 0.0685 - accuracy: 0.9793 - val_loss: 0.0413 - val_accuracy: 0.9882\n",
            "Epoch 5/15\n",
            "422/422 [==============================] - 4s 8ms/step - loss: 0.0632 - accuracy: 0.9803 - val_loss: 0.0401 - val_accuracy: 0.9887\n",
            "Epoch 6/15\n",
            "422/422 [==============================] - 3s 8ms/step - loss: 0.0564 - accuracy: 0.9821 - val_loss: 0.0357 - val_accuracy: 0.9907\n",
            "Epoch 7/15\n",
            "422/422 [==============================] - 3s 8ms/step - loss: 0.0514 - accuracy: 0.9839 - val_loss: 0.0341 - val_accuracy: 0.9908\n",
            "Epoch 8/15\n",
            "422/422 [==============================] - 3s 8ms/step - loss: 0.0493 - accuracy: 0.9844 - val_loss: 0.0323 - val_accuracy: 0.9902\n",
            "Epoch 9/15\n",
            "422/422 [==============================] - 3s 8ms/step - loss: 0.0453 - accuracy: 0.9856 - val_loss: 0.0319 - val_accuracy: 0.9908\n",
            "Epoch 10/15\n",
            "422/422 [==============================] - 4s 8ms/step - loss: 0.0415 - accuracy: 0.9872 - val_loss: 0.0315 - val_accuracy: 0.9918\n",
            "Epoch 11/15\n",
            "422/422 [==============================] - 3s 8ms/step - loss: 0.0414 - accuracy: 0.9867 - val_loss: 0.0286 - val_accuracy: 0.9920\n",
            "Epoch 12/15\n",
            "422/422 [==============================] - 4s 8ms/step - loss: 0.0376 - accuracy: 0.9878 - val_loss: 0.0294 - val_accuracy: 0.9922\n",
            "Epoch 13/15\n",
            "422/422 [==============================] - 3s 8ms/step - loss: 0.0364 - accuracy: 0.9886 - val_loss: 0.0275 - val_accuracy: 0.9922\n",
            "Epoch 14/15\n",
            "422/422 [==============================] - 4s 8ms/step - loss: 0.0346 - accuracy: 0.9889 - val_loss: 0.0289 - val_accuracy: 0.9920\n",
            "Epoch 15/15\n",
            "422/422 [==============================] - 4s 8ms/step - loss: 0.0327 - accuracy: 0.9896 - val_loss: 0.0306 - val_accuracy: 0.9918\n",
            "Test loss: 0.026304325088858604\n",
            "Test accuracy: 0.9908999800682068\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U2iSr5ByhRD5"
      },
      "source": [
        "# DATA GENERATORS"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uBcX9tbu10U0"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vsdLACjQh-DU"
      },
      "source": [
        "# dir(gen)\n",
        "# \n",
        "#  'flow',\n",
        "#  'flow_from_dataframe',\n",
        "#  'flow_from_directory'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ljcOSI801oTX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "outputId": "ef743d2f-4d34-4512-c621-ada1fb951ecc"
      },
      "source": [
        "\n",
        "model = keras.Sequential(\n",
        "    [\n",
        "        keras.Input(shape=input_shape),\n",
        "        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
        "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
        "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        layers.Flatten(),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(num_classes, activation=\"softmax\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"], lr=0.01)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-183cc5fc7889>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m )\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"categorical_crossentropy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"adam\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"accuracy\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mcompile\u001b[0;34m(self, optimizer, loss, metrics, loss_weights, weighted_metrics, run_eagerly, steps_per_execution, **kwargs)\u001b[0m\n\u001b[1;32m    568\u001b[0m       \u001b[0mfrom_serialized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'from_serialized'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_compile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_eagerly\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_eagerly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_validate_compile\u001b[0;34m(self, optimizer, metrics, **kwargs)\u001b[0m\n\u001b[1;32m   2619\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minvalid_kwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2620\u001b[0m       raise TypeError('Invalid keyword argument(s) in `compile`: %s' %\n\u001b[0;32m-> 2621\u001b[0;31m                       (invalid_kwargs,))\n\u001b[0m\u001b[1;32m   2622\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2623\u001b[0m     \u001b[0;31m# Model must be created and compiled with the same DistStrat.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Invalid keyword argument(s) in `compile`: {'lr'}"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ywfr9QeMilC4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "759de737-5650-44d9-f24d-923ad288ad70"
      },
      "source": [
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "num_epochs = 20\n",
        "batch_size = 128\n",
        "num_classes = 10\n",
        "input_shape = (28, 28, 1)\n",
        "\n",
        "model = keras.Sequential(\n",
        "    [\n",
        "        keras.Input(shape=input_shape),\n",
        "        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
        "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
        "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        layers.Flatten(),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(num_classes, activation=\"softmax\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "\n",
        "# test with actual mnist\n",
        "# todo: std\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "\n",
        "x_train =  np.expand_dims(x_train, -1)\n",
        "x_test =  np.expand_dims(x_test, -1)\n",
        "\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "gen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1/255.)\n",
        "    # samplewise_std_normalization=True)\n",
        "    # featurewise_center=True,\n",
        "    # featurewise_std_normalization=True,)\n",
        "    # rotation_range=20,\n",
        "    # width_shift_range=0.2,\n",
        "    # height_shift_range=0.2,\n",
        "    # horizontal_flip=True,\n",
        "    #validation_split=0.2)\n",
        "\n",
        "gen_train = gen.flow(x_train, y_train, batch_size=batch_size, shuffle=True)\n",
        "gen_test = gen.flow(x_test, y_test, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "print('steps_per_epoch=',len(gen_train)//batch_size)\n",
        "print('per step:', (len(gen_train)//batch_size) * batch_size)\n",
        "\n",
        "fit_stats = model.fit(gen_train,\n",
        "                      steps_per_epoch=len(gen_train), # total batches of samples. gen_train is already divided into batches of samples.\n",
        "                      epochs=num_epochs,\n",
        "                      validation_data=gen_test,\n",
        "                      validation_steps=len(gen_test)) # total batches of samples. gen_test is already divided into batches of samples"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n",
            "steps_per_epoch= 3\n",
            "per step: 384\n",
            "Epoch 1/20\n",
            "469/469 [==============================] - 6s 11ms/step - loss: 0.3456 - accuracy: 0.8966 - val_loss: 0.0858 - val_accuracy: 0.9760\n",
            "Epoch 2/20\n",
            "469/469 [==============================] - 5s 10ms/step - loss: 0.1007 - accuracy: 0.9691 - val_loss: 0.0535 - val_accuracy: 0.9835\n",
            "Epoch 3/20\n",
            "469/469 [==============================] - 5s 10ms/step - loss: 0.0753 - accuracy: 0.9771 - val_loss: 0.0423 - val_accuracy: 0.9857\n",
            "Epoch 4/20\n",
            "469/469 [==============================] - 5s 10ms/step - loss: 0.0644 - accuracy: 0.9797 - val_loss: 0.0373 - val_accuracy: 0.9880\n",
            "Epoch 5/20\n",
            "469/469 [==============================] - 5s 10ms/step - loss: 0.0562 - accuracy: 0.9829 - val_loss: 0.0308 - val_accuracy: 0.9885\n",
            "Epoch 6/20\n",
            "469/469 [==============================] - 5s 10ms/step - loss: 0.0501 - accuracy: 0.9843 - val_loss: 0.0298 - val_accuracy: 0.9895\n",
            "Epoch 7/20\n",
            "469/469 [==============================] - 5s 10ms/step - loss: 0.0476 - accuracy: 0.9851 - val_loss: 0.0288 - val_accuracy: 0.9900\n",
            "Epoch 8/20\n",
            "469/469 [==============================] - 5s 10ms/step - loss: 0.0431 - accuracy: 0.9870 - val_loss: 0.0270 - val_accuracy: 0.9904\n",
            "Epoch 9/20\n",
            "469/469 [==============================] - 5s 10ms/step - loss: 0.0419 - accuracy: 0.9865 - val_loss: 0.0291 - val_accuracy: 0.9901\n",
            "Epoch 10/20\n",
            "469/469 [==============================] - 4s 10ms/step - loss: 0.0390 - accuracy: 0.9880 - val_loss: 0.0254 - val_accuracy: 0.9912\n",
            "Epoch 11/20\n",
            "469/469 [==============================] - 5s 10ms/step - loss: 0.0354 - accuracy: 0.9886 - val_loss: 0.0285 - val_accuracy: 0.9902\n",
            "Epoch 12/20\n",
            "469/469 [==============================] - 4s 10ms/step - loss: 0.0341 - accuracy: 0.9894 - val_loss: 0.0257 - val_accuracy: 0.9916\n",
            "Epoch 13/20\n",
            "469/469 [==============================] - 5s 10ms/step - loss: 0.0331 - accuracy: 0.9893 - val_loss: 0.0269 - val_accuracy: 0.9914\n",
            "Epoch 14/20\n",
            "469/469 [==============================] - 5s 10ms/step - loss: 0.0317 - accuracy: 0.9895 - val_loss: 0.0235 - val_accuracy: 0.9921\n",
            "Epoch 15/20\n",
            "469/469 [==============================] - 4s 10ms/step - loss: 0.0294 - accuracy: 0.9906 - val_loss: 0.0246 - val_accuracy: 0.9916\n",
            "Epoch 16/20\n",
            "469/469 [==============================] - 5s 10ms/step - loss: 0.0282 - accuracy: 0.9905 - val_loss: 0.0233 - val_accuracy: 0.9923\n",
            "Epoch 17/20\n",
            "469/469 [==============================] - 4s 9ms/step - loss: 0.0292 - accuracy: 0.9908 - val_loss: 0.0237 - val_accuracy: 0.9925\n",
            "Epoch 18/20\n",
            "469/469 [==============================] - 4s 9ms/step - loss: 0.0264 - accuracy: 0.9915 - val_loss: 0.0235 - val_accuracy: 0.9923\n",
            "Epoch 19/20\n",
            "469/469 [==============================] - 4s 9ms/step - loss: 0.0268 - accuracy: 0.9910 - val_loss: 0.0240 - val_accuracy: 0.9925\n",
            "Epoch 20/20\n",
            "469/469 [==============================] - 4s 9ms/step - loss: 0.0255 - accuracy: 0.9915 - val_loss: 0.0234 - val_accuracy: 0.9932\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_dxnUBBt3nqa"
      },
      "source": [
        "tf.keras.preprocessing.image.ImageDataGenerator?"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tLM65q5M3pCM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 578
        },
        "outputId": "a319dd6d-17d8-4dbd-d719-d90ff34fd753"
      },
      "source": [
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "num_epochs = 20\n",
        "batch_size = 128\n",
        "num_classes = 10\n",
        "input_shape = (28, 28, 1)\n",
        "\n",
        "model = keras.Sequential(\n",
        "    [\n",
        "        keras.Input(shape=input_shape),\n",
        "        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
        "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
        "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        layers.Flatten(),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(num_classes, activation=\"softmax\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "\n",
        "# test with actual mnist\n",
        "# todo: std\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "\n",
        "x_train =  np.expand_dims(x_train, -1)\n",
        "x_test =  np.expand_dims(x_test, -1)\n",
        "\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "gen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1/255.)\n",
        "    # samplewise_std_normalization=True)\n",
        "    # featurewise_center=True,\n",
        "    # featurewise_std_normalization=True,)\n",
        "    # rotation_range=20,\n",
        "    # width_shift_range=0.2,\n",
        "    # height_shift_range=0.2,\n",
        "    # horizontal_flip=True,\n",
        "    #validation_split=0.2)\n",
        "\n",
        "gen_train = gen.flow(x_train, y_train, batch_size=batch_size, shuffle=True)\n",
        "gen_test = gen.flow(x_test, y_test, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "print('steps_per_epoch=',len(gen_train)//batch_size)\n",
        "print('per step:', (len(gen_train)//batch_size) * batch_size)\n",
        "\n",
        "\n",
        "for i in range(15):\n",
        "  # there is overhead with `fit`. use `train_on_batch` instead.\n",
        "  fit_stats = model.fit(gen_train,\n",
        "                        steps_per_epoch=len(gen_train), # total batches of samples. gen_train is already divided into batches of samples.\n",
        "                        epochs=1,\n",
        "                        validation_data=gen_test,\n",
        "                        validation_steps=len(gen_test)) # total batches of samples. gen_test is already divided into batches of samples\n",
        "\n",
        "  print(i, fit_stats.history)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n",
            "steps_per_epoch= 3\n",
            "per step: 384\n",
            "469/469 [==============================] - 6s 11ms/step - loss: 0.3534 - accuracy: 0.8928 - val_loss: 0.0834 - val_accuracy: 0.9761\n",
            "0 {'loss': [0.3533603847026825], 'accuracy': [0.8927500247955322], 'val_loss': [0.08341821283102036], 'val_accuracy': [0.9761000275611877]}\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.1043 - accuracy: 0.9686 - val_loss: 0.0532 - val_accuracy: 0.9838\n",
            "1 {'loss': [0.1042555570602417], 'accuracy': [0.9685666561126709], 'val_loss': [0.05319143086671829], 'val_accuracy': [0.9837999939918518]}\n",
            "469/469 [==============================] - 5s 12ms/step - loss: 0.0792 - accuracy: 0.9754 - val_loss: 0.0434 - val_accuracy: 0.9856\n",
            "2 {'loss': [0.07919009029865265], 'accuracy': [0.975433349609375], 'val_loss': [0.04342900961637497], 'val_accuracy': [0.9855999946594238]}\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.0644 - accuracy: 0.9800 - val_loss: 0.0393 - val_accuracy: 0.9876\n",
            "3 {'loss': [0.06441231817007065], 'accuracy': [0.9800166487693787], 'val_loss': [0.039346855133771896], 'val_accuracy': [0.9876000285148621]}\n",
            "467/469 [============================>.] - ETA: 0s - loss: 0.0585 - accuracy: 0.9817"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-58c7e9eb8782>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     58\u001b[0m                         \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m                         \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgen_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m                         validation_steps=len(gen_test)) # total batches of samples. gen_test is already divided into batches of samples\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit_stats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1223\u001b[0m               \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1224\u001b[0m               \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1225\u001b[0;31m               _use_cached_eval_dataset=True)\n\u001b[0m\u001b[1;32m   1226\u001b[0m           \u001b[0mval_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'val_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mval_logs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1227\u001b[0m           \u001b[0mepoch_logs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m   1487\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_r\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1488\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1489\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1490\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1491\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    922\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 924\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    925\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3022\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   3023\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 3024\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   3025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3026\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1959\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1960\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1961\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1962\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1963\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    594\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    597\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "798pUT2nC1hq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19ee40c8-88f7-42e3-d68e-6b3dcee50476"
      },
      "source": [
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "num_epochs = 20\n",
        "batch_size = 128\n",
        "num_classes = 10\n",
        "input_shape = (28, 28, 1)\n",
        "\n",
        "model = keras.Sequential(\n",
        "    [\n",
        "        keras.Input(shape=input_shape),\n",
        "        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
        "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
        "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        layers.Flatten(),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(num_classes, activation=\"softmax\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "\n",
        "# test with actual mnist\n",
        "# todo: std\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "\n",
        "x_train =  np.expand_dims(x_train, -1)\n",
        "x_test =  np.expand_dims(x_test, -1)\n",
        "\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "gen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1/255.)\n",
        "    # samplewise_std_normalization=True)\n",
        "    # featurewise_center=True,\n",
        "    # featurewise_std_normalization=True,)\n",
        "    # rotation_range=20,\n",
        "    # width_shift_range=0.2,\n",
        "    # height_shift_range=0.2,\n",
        "    # horizontal_flip=True,\n",
        "    #validation_split=0.2)\n",
        "\n",
        "gen_train = gen.flow(x_train, y_train, batch_size=batch_size, shuffle=True)\n",
        "gen_test = gen.flow(x_test, y_test, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "print('steps_per_epoch=',len(gen_train)//batch_size)\n",
        "print('per step:', (len(gen_train)//batch_size) * batch_size)\n",
        "\n",
        "\n",
        "for epoch in range(15):\n",
        "  epoch_losses_train, epoch_accs_train = [], []\n",
        "  for batch_idx, (xs_batch, ys_batch) in enumerate(gen_train):\n",
        "    dic = model.train_on_batch(xs_batch, ys_batch, sample_weight=None, class_weight=None, reset_metrics=True, return_dict=True,)\n",
        "\n",
        "    epoch_losses_train.append(dic['loss'])\n",
        "    epoch_accs_train.append(dic['accuracy'])\n",
        "    \n",
        "    if batch_idx > len(gen_train):\n",
        "      break\n",
        "\n",
        "  print(f'train@epoch{epoch} :: loss ::', np.mean(epoch_losses_train), ':: acc ::', np.median(epoch_accs_train), end=\"\\t\\t\")\n",
        "\n",
        "  val_stats = model.evaluate_generator(gen_test, steps=len(gen_test))\n",
        "  print(f'val@epoch{epoch}:', val_stats)\n",
        "\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n",
            "steps_per_epoch= 3\n",
            "per step: 384\n",
            "train@epoch0 :: loss :: 0.33794779336028546 :: acc :: 0.9375\t\t"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1973: UserWarning: `Model.evaluate_generator` is deprecated and will be removed in a future version. Please use `Model.evaluate`, which supports generators.\n",
            "  warnings.warn('`Model.evaluate_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "val@epoch0: [0.08780309557914734, 0.9747999906539917]\n",
            "train@epoch1 :: loss :: 0.10915106421854592 :: acc :: 0.96875\t\tval@epoch1: [0.054930657148361206, 0.9830999970436096]\n",
            "train@epoch2 :: loss :: 0.08129966397811274 :: acc :: 0.9765625\t\tval@epoch2: [0.046488020569086075, 0.9853000044822693]\n",
            "train@epoch3 :: loss :: 0.06771053659537858 :: acc :: 0.9765625\t\tval@epoch3: [0.039375606924295425, 0.9872000217437744]\n",
            "train@epoch4 :: loss :: 0.05875998753718162 :: acc :: 0.984375\t\tval@epoch4: [0.03310172259807587, 0.9889000058174133]\n",
            "train@epoch5 :: loss :: 0.05519542083413105 :: acc :: 0.984375\t\tval@epoch5: [0.03249957412481308, 0.9897000193595886]\n",
            "train@epoch6 :: loss :: 0.04804942122959154 :: acc :: 0.984375\t\tval@epoch6: [0.032309819012880325, 0.9886999726295471]\n",
            "train@epoch7 :: loss :: 0.04618011940677992 :: acc :: 0.984375\t\tval@epoch7: [0.03142884001135826, 0.9897000193595886]\n",
            "train@epoch8 :: loss :: 0.04228646163838558 :: acc :: 0.984375\t\tval@epoch8: [0.02987399511039257, 0.9901999831199646]\n",
            "train@epoch9 :: loss :: 0.04116391524031853 :: acc :: 0.9921875\t\tval@epoch9: [0.02952614054083824, 0.9898999929428101]\n",
            "train@epoch10 :: loss :: 0.03863866443126888 :: acc :: 0.9921875\t\tval@epoch10: [0.025101669132709503, 0.9911999702453613]\n",
            "train@epoch11 :: loss :: 0.035700426757288205 :: acc :: 0.9921875\t\tval@epoch11: [0.026723232120275497, 0.9908999800682068]\n",
            "train@epoch12 :: loss :: 0.034144836723124274 :: acc :: 0.9921875\t\tval@epoch12: [0.025926552712917328, 0.991100013256073]\n",
            "train@epoch13 :: loss :: 0.033064483219536214 :: acc :: 0.9921875\t\tval@epoch13: [0.024362793192267418, 0.9916999936103821]\n",
            "train@epoch14 :: loss :: 0.03183286817795785 :: acc :: 0.9921875\t\tval@epoch14: [0.02506173774600029, 0.991100013256073]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8UnzLNlhHGo6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}